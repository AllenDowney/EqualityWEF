{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import decorate, configure_plot_style, AIBM_COLORS\n",
    "\n",
    "configure_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_iso_dict = {\n",
    "    'Austria': 'AUT',\n",
    "    'Canada': 'CAN',\n",
    "    'Chile': 'CHL',\n",
    "    'Czechia': 'CZE',\n",
    "    'Denmark': 'DNK',\n",
    "    'Estonia': 'EST',\n",
    "    'Finland': 'FIN',\n",
    "    'France': 'FRA',\n",
    "    'Germany': 'DEU',\n",
    "    'Hungary': 'HUN',\n",
    "    'Ireland': 'IRL',\n",
    "    'Israel': 'ISR',\n",
    "    'Italy': 'ITA',\n",
    "    'Japan': 'JPN',\n",
    "    'Korea': 'KOR',\n",
    "    'Latvia': 'LVA',\n",
    "    'Lithuania': 'LTU',\n",
    "    'Netherlands': 'NLD',\n",
    "    'New Zealand': 'NZL',\n",
    "    'Norway': 'NOR',\n",
    "    'Poland*': 'POL',\n",
    "    'Portugal': 'PRT',\n",
    "    'Slovak Republic': 'SVK',\n",
    "    'Spain': 'ESP',\n",
    "    'Sweden': 'SWE',\n",
    "    'Switzerland': 'CHE',\n",
    "    'United States': 'USA'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literacy\n",
    "\n",
    "From the WEF report (emphasis added):\n",
    "\n",
    "> Literacy rate, %\n",
    ">\n",
    ">Percentage of the adult population (women and men over 15 years of age) with the ability to\n",
    "both read and write and make simple arithmetic calculations. *For advanced economies for which\n",
    "data was unavailable in the last 10 years, the authors assumed based on older data that the\n",
    "gender gap on literacy rate is closed.* \n",
    ">\n",
    ">Period: 2023 or most recent year available. Source: UNESCO, UIS.Stat education statistics\n",
    "data portal; when not available, data is sourced from the UNDP Human Development Reports, most\n",
    "recent data available.\n",
    "\n",
    "\n",
    "UNESCO stopped tracking OECD countries a while ago (more than 10 years, it seems, based on OWID data). So the WEF gives them a pass. But in many of those countries, women now surpass men in literacy, as we can see in data from the OEDC Survey of Adult Skills (PIAAC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our World In Data\n",
    "\n",
    "Lightly processed data from UNESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the data.\n",
    "owid = pd.read_csv(\"https://ourworldindata.org/grapher/literacy-rate-of-young-men-and-women.csv?v=1&csvType=full&useColumnShortNames=true\", storage_options = {'User-Agent': 'Our World In Data data fetch/1.0'})\n",
    "\n",
    "# Fetch the metadata\n",
    "metadata = requests.get(\"https://ourworldindata.org/grapher/literacy-rate-of-young-men-and-women.metadata.json?v=1&csvType=full&useColumnShortNames=true\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid.columns = ['Entity', 'Code', 'Year',\n",
    "       'literacy_male',\n",
    "       'literacy_female',\n",
    "       'owid_region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid.query(\"Code == 'AFG'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last = owid.groupby([\"Code\"]).last()\n",
    "owid_last['ratio'] = owid_last['literacy_female'] / owid_last['literacy_male']\n",
    "owid_last['score'] = 1 - np.abs(owid_last['ratio'] - 1)\n",
    "owid_last.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last.sort_values(by='score', ascending=False).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = owid_last['score'].isna()\n",
    "owid_last[missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last['score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OECD PIAAC\n",
    "\n",
    "From https://www.oecd.org/en/about/programmes/piaac.html\n",
    "\n",
    ">The Programme for the International Assessment of Adult Competencies (PIAAC) is a programme of assessment and analysis of adult skills. The major product of PIAAC is the Survey of Adult Skills, an international computer-based household survey of adults aged 16-65 years. It is designed as 10-yearly cycles.\n",
    "\n",
    ">The Survey measures adults’ proficiency in key information-processing skills - literacy, numeracy and problem solving – which represent skills needed for individuals to participate in society and for economies to prosper. It also gathers information and data on how adults use their skills at home and at work.\n",
    "\n",
    ">The 1st Cycle of the Survey of Adult Skills was conducted over three separate rounds between 2011 and 2018 in 39 countries. During the 1st Cycle, about 245 000 adults were interviewed, representing 1.15 billion people.\n",
    "\n",
    ">The 2nd Cycle of the Survey of Adults Skills has been conducted in 31 countries and economies so far. A first round of data collection took place in 2022-2023 with results released on 10 December 2024.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I downloaded two tables from https://www.oecd.org/en/publications/do-adults-have-the-skills-they-need-to-thrive-in-a-changing-world_b263dc5d-en/full-report/tables-of-results-for-countries-and-economies_0432d7e4.html#annex-d1e19379-dc605a08b8:\n",
    "\n",
    "* Table A.2.7 (L): Literacy proficiency, by gender\n",
    "\n",
    "* Table A.2.8 (L): Literacy proficiency, by gender and age group\n",
    "\n",
    "Both are sheets in this Excel spreadsheet: https://stat.link/eb8dxq\n",
    "\n",
    "The first table includes \"Percentage of low performers (scoring at Level 1 or below)\", which we will use as a replacement for illiteracy.\n",
    "\n",
    "Here's the description of Level 1:\n",
    "\n",
    "> Adults at Level 1 are able to locate information on a text page, find a relevant link from a website, and identify relevant text among multiple options when the relevant information is explicitly cued. They can understand the meaning of short texts, as well as the organization of lists or multiple sections within a single page.\n",
    ">\n",
    ">The texts at level 1 may be continuous, noncontinuous, or mixed and pertain to printed or digital environments. They typically include a single page with up to a few hundred words and little or no distracting information. Noncontinuous texts may have a list structure (such as a web search engine results page) or include a small number of independent sections, possibly with pictorial illustrations or simple diagrams. Tasks at Level 1 involve simple questions providing some guidance as to what needs to be done and a single processing step. There is a direct, fairly obvious match between the question and target information in the text, although some tasks may require the examination of more than one piece of information.\n",
    "\n",
    "We'll flip the sense by computing:\n",
    "\n",
    "`literacy rate = (100 - Percentage of low performers)`\n",
    "\n",
    "Here's the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piaac = pd.read_excel('eb8dxq.xlsx', sheet_name='A.2.7 (L)', skiprows=6, skipfooter=11)\n",
    "\n",
    "piaac.columns = ['country', 'mean', 'se', \n",
    "           'male_mean', 'male_mean_se', 'female_mean', 'female_mean_se',\n",
    "           'diff', 'diff_se', \n",
    "           'unused', 'unused', 'unused', 'unused', \n",
    "           'male_percent', 'male_percent_se', 'female_percent', 'female_percent_se', \n",
    "           'unused', 'unused']\n",
    "\n",
    "piaac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piaac['code'] = piaac['country'].map(country_to_iso_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the ratio intended to be comparable to the WEF equity scores, except that it's symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piaac['ratio'] = (100 - piaac['female_percent']) / (100 - piaac['male_percent'])\n",
    "piaac['ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piaac['score'] = 1 - np.abs(piaac['ratio'] - 1)\n",
    "piaac['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_title, add_subtext, add_logo\n",
    "\n",
    "piaac_sorted = piaac.sort_values(by='female_percent')\n",
    "country = piaac_sorted['country']\n",
    "male = 100 - piaac_sorted['male_percent']\n",
    "female = 100 - piaac_sorted['female_percent']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "plt.hlines(country, male, female, color=AIBM_COLORS['light_gray'])\n",
    "plt.plot(male, country, 's', color=AIBM_COLORS['green'], label='Male')\n",
    "plt.plot(female, country, 'o', color=AIBM_COLORS['orange'], label='Female')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "decorate(xlabel='Percent performing above Level 1', ylim=[len(piaac), 0.5])\n",
    "add_title(\"In Most OECD Countries Men Lag Women in Literacy\",\n",
    "          \"Based on the PIAAC Survey of Adult Skills\", y=1.01)\n",
    "add_subtext(\"Source: OECD PIAAC\", y=-0.05)\n",
    "logo = add_logo(location=(1.0, -0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIACC by age and gender\n",
    "\n",
    "The percentages in the previous table include all adults, so they are comparable to WEF literacy percentages in that sense.\n",
    "But adult literacy is a long-lagging indicator of equity in primary and secondary education. To get a sense of generational shifts, we can split the PIAAC data by age group, this time using average scores rather than percentages above or below Level 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piaac2 = pd.read_excel('eb8dxq.xlsx', sheet_name='A.2.8 (L)', skiprows=7, skipfooter=11)\n",
    "piaac2.columns = ['country', 'mean', 'se', \n",
    "              'male_1624', 'unused', \n",
    "              'male_2544', 'unused',\n",
    "              'male_4565', 'unused', \n",
    "              'female_1624', 'unused', \n",
    "              'female_2544', 'unused', \n",
    "              'female_4565', 'unused']\n",
    "piaac2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_1624 = piaac2['female_1624'] - piaac2['male_1624']\n",
    "diff_2544 = piaac2['female_2544'] - piaac2['male_2544']\n",
    "diff_4565 = piaac2['female_4565'] - piaac2['male_4565']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diffs = pd.DataFrame(dict(\n",
    "    country=piaac2['country'],\n",
    "    diff_1624=diff_1624,\n",
    "    diff_2544=diff_2544,\n",
    "    diff_4565=diff_4565))\n",
    "diffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_title, add_subtext, add_logo\n",
    "\n",
    "diffs_sorted = diffs.sort_values(by='diff_1624', ascending=False)\n",
    "country = diffs_sorted['country']\n",
    "young = diffs_sorted['diff_1624']\n",
    "middle = diffs_sorted['diff_2544']\n",
    "old = diffs_sorted['diff_4565']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "plt.hlines(country, young, middle, color=AIBM_COLORS['light_gray'])\n",
    "plt.hlines(country, old, middle, color=AIBM_COLORS['light_gray'])\n",
    "\n",
    "plt.plot(young, country, '>', \n",
    "         color=AIBM_COLORS['blue'], label='Ages 16-24')\n",
    "plt.plot(middle, country, 'o', \n",
    "         color=AIBM_COLORS['purple'], label='Ages 25-44')\n",
    "plt.plot(old, country, '<', \n",
    "         color=AIBM_COLORS['amber'], label='Ages 45-65')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "decorate(xlabel='Gender gap in average literacy scores', ylim=[len(piaac2), 0.5])\n",
    "add_title(\"In Some Countries Gaps Are Widest Among the Young\",\n",
    "          \"\", y=1.01)\n",
    "add_subtext(\"Source: OECD PIAAC\", y=-0.05)\n",
    "logo = add_logo(location=(1.0, -0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some countries the gender gap is widest among young people.\n",
    "In these countries, the use of a lagging indicator might understate the degree of inequality.\n",
    "\n",
    "Not to make too much of this point -- it is probably a reasonable thing to include in the index one measure that is an aggregation of the entire population, along with snapshots of current education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge PIACC into OWID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last['merged_score'] = owid_last['score']\n",
    "for i, row in piaac.iterrows():\n",
    "    code = row['code']\n",
    "    if code in owid_last.index:\n",
    "        owid_last.at[code, 'merged_score'] = row['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_codes = set(owid['Code'])\n",
    "piaac_codes = set(piaac['code'])\n",
    "overlap = owid_codes & piaac_codes\n",
    "len(owid_codes), len(piaac_codes), len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_last.query(\"Code in @overlap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from extract_pdf_data import read_pdfs\n",
    "\n",
    "if not os.path.exists(\"wef_literacy_rate.csv\"):\n",
    "    wef = read_pdfs('literacy')\n",
    "    wef.to_csv(\"wef_literacy_rate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_wef_file\n",
    "\n",
    "wef = read_wef_file(\"wef_literacy_rate.csv\")\n",
    "wef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wef.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wef['rank'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinged = wef['score'] < 1\n",
    "dinged.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Cdf\n",
    "\n",
    "cdf_score = Cdf.from_seq(wef.loc[dinged, 'score'])\n",
    "cdf_score.plot(label='WEF')\n",
    "decorate(xlabel='Literacy Score', ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import iso_country_dict\n",
    "\n",
    "country_to_iso_dict = {}\n",
    "for code, country in iso_country_dict.items():\n",
    "    country_to_iso_dict[country] = code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in wef.iterrows():\n",
    "    code = country_to_iso_dict[row['country']]\n",
    "    if code in owid_last.index:\n",
    "        wef.at[code, 'revised_score'] = owid_last.loc[code, 'merged_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wef.sort_values(by='revised_score', ascending=False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the distribution of revised scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_score.plot(label='WEF')\n",
    "cdf_revised = Cdf.from_seq(wef['revised_score'])\n",
    "cdf_revised.plot(label='WEF (Revised)')\n",
    "decorate(xlabel='Revised Literacy Score', ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised = df[~dinged].dropna(subset=['ratio']).sort_values('revised_score')\n",
    "revised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure that shows a horizontal line for each country that\n",
    "# connects the revised score and the secondary enrolment score\n",
    "# with a circle for the original and a triangle for the revised\n",
    "\n",
    "from utils import add_title, add_subtext, add_logo\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 15))\n",
    "plt.hlines(revised['country'], \n",
    "           revised['score'], revised['revised_score'], \n",
    "           color=AIBM_COLORS['light_gray'])\n",
    "plt.plot(revised['score'], revised['country'], '|', \n",
    "         color=AIBM_COLORS['blue'])\n",
    "plt.plot(revised['revised_score'], revised['country'], '<', \n",
    "         color=AIBM_COLORS['blue'])\n",
    "ax.invert_yaxis()\n",
    "\n",
    "decorate(xlabel='Literacy Rate', ylim=[len(revised)+1, -1])\n",
    "add_title(\"Revised Scores Are Very Different For Many Countries\",\n",
    "          \"Literacy rate\", y=1.01)\n",
    "add_subtext(\"Source: World Economic Forum\", y=-0.05)\n",
    "logo = add_logo(location=(1.0, -0.05))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the new ranking of countries based on revised scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['revised_rank', 'country', 'score', 'revised_score', 'diff']\n",
    "df_sorted = df.dropna(subset=['ratio']).sort_values(by='revised_score', ascending=False)\n",
    "df_sorted['revised_rank'] = np.arange(1, len(df_sorted)+1)\n",
    "table = df_sorted[columns].round(2)\n",
    "table.to_csv(\"wef_literacy_rate_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sorted[columns].head(40).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted[columns].tail(50).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"country == 'Qatar'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "options = dict(cut=0, bw_adjust=0.7)\n",
    "\n",
    "sns.kdeplot(df['score'], label='WEF truncated scores', **options)\n",
    "sns.kdeplot(df['revised_score'], label='Revised symmetric scores', **options)\n",
    "\n",
    "decorate(xlabel='Gender equality score')\n",
    "\n",
    "add_title(\"The Distribution of Scores Is Very Different\",\n",
    "          \"Literacy rate\")\n",
    "add_subtext(\"Source: World Economic Forum\", y=-0.25)\n",
    "logo = add_logo(location=(1.0, -0.25))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNESCO Data\n",
    "\n",
    "UNESCO, UIS.Stat Education statistics data portal. When not available, data is sourced from United Nations Development Programme, Human Development Reports, most recent year available between 2013 and 2023.\n",
    "\n",
    "https://databrowser.uis.unesco.org/view#=countries&geoUnits=&timeMode=range&view=table&chartMode=multiple&chartHighlightSeries=&chartHighlightEnabled=true&indicatorPaths=UIS-SDG4Monitoring%3A0%3ALR.GALP.AG15T24.F%2CUIS-SDG4Monitoring%3A0%3ALR.GALP.AG15T24.M%2CUIS-SDG4Monitoring%3A0%3ALR.GALP.AG15T99.F%2CUIS-SDG4Monitoring%3A0%3ALR.GALP.AG15T99.M%2CUIS-SDG4Monitoring%3A0%3ALR.GALP.AG25T64.F%2CUIS-SDG4Monitoring%3A0%3ALR.GALP.AG25T64.M&geoMode=countries&tableIndicatorId=LR.GALP.AG15T99.F&years=2013%2C2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"indicator-data-export_LR.GALP.AG15T24.F_LR.GALP.AG15T24.M_LR.GALP.AG15T99.F_and_3_more/data.csv\"\n",
    "\n",
    "unesco = pd.read_csv(filename)\n",
    "unesco.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import iso_country_dict\n",
    "\n",
    "country_map = pd.Series(iso_country_dict)\n",
    "unesco['country'] = unesco['geoUnit'].map(country_map)\n",
    "unesco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unesco.query('geoUnit == \"CAN\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from each group, select the row with the latest date\n",
    "grouped = unesco.groupby([\"indicatorId\", \"geoUnit\"]).last()\n",
    "grouped.loc[:, 'AFG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'youth'\n",
    "\n",
    "if group == 'adults':\n",
    "    age_group = 'AG25T64'\n",
    "elif group == 'youth':\n",
    "    age_group = 'AG15T24'\n",
    "\n",
    "\n",
    "male = grouped.loc[f'LR.GALP.{age_group}.M']\n",
    "male.index = male['country']\n",
    "female = grouped.loc[f'LR.GALP.{age_group}.F']\n",
    "female.index = female['country']\n",
    "\n",
    "male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literacy = pd.concat([male, female], axis=1, keys=[\"male\", \"female\"])\n",
    "literacy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the ratio for each country in the literacy dataframe\n",
    "\n",
    "df['unesco_ratio'] = literacy['ratio'].reindex(df['country']).values\n",
    "df[['country', 'score', 'unesco_ratio']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EqualityWEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
